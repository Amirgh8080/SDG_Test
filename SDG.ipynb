{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPJ8Yxn1oCyWARHKx5oOq2i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amirgh8080/SDG_Test/blob/main/SDG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the implmentation of the SDG model"
      ],
      "metadata": {
        "id": "bqiu9AHXXmdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Preparing the VM"
      ],
      "metadata": {
        "id": "anJSMntwZD98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing the nessassaries libraries suggested by the paper's authors"
      ],
      "metadata": {
        "id": "bQXL1DC4X2uu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI_etiNhQwNk",
        "outputId": "1fbc3857-a0b2-4a9e-be1d-802685b6fc40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.14.6\n",
            "  Downloading numpy-1.14.6.zip (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scipy==1.2.1\n",
            "  Downloading scipy-1.2.1.tar.gz (23.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch==1.6.0 (from versions: 0.1.2, 1.0.2)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pytorch==1.6.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.14.6 scipy==1.2.1 pytorch==1.6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting the model files"
      ],
      "metadata": {
        "id": "0l6hV5UsYGfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip SDG-main.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7ZuLiM8RZ4I",
        "outputId": "d6ecff99-3940-48c0-d99e-da07121112a6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  SDG-main.zip\n",
            "7a09ae77466aca8ecb68300104d6880fab295412\n",
            "   creating: SDG-main/\n",
            "  inflating: SDG-main/README.md      \n",
            "   creating: SDG-main/code/\n",
            "  inflating: SDG-main/code/README.md  \n",
            "  inflating: SDG-main/code/main.py   \n",
            "   creating: SDG-main/code/sdg/\n",
            "   creating: SDG-main/code/sdg/data/\n",
            "  inflating: SDG-main/code/sdg/data/citeseer.npz  \n",
            "  inflating: SDG-main/code/sdg/data/cora_ml.npz  \n",
            "  inflating: SDG-main/code/sdg/data/io.py  \n",
            "  inflating: SDG-main/code/sdg/data/pubmed.npz  \n",
            "  inflating: SDG-main/code/sdg/data/sparsegraph.py  \n",
            "   creating: SDG-main/code/sdg/pytorch_code/\n",
            "  inflating: SDG-main/code/sdg/pytorch_code/agnostic_model.py  \n",
            "  inflating: SDG-main/code/sdg/pytorch_code/earlystopping.py  \n",
            "  inflating: SDG-main/code/sdg/pytorch_code/preprocessing.py  \n",
            "  inflating: SDG-main/code/sdg/pytorch_code/propagation.py  \n",
            "  inflating: SDG-main/code/sdg/pytorch_code/training.py  \n",
            "  inflating: SDG-main/code/sdg/pytorch_code/utils.py  \n",
            "   creating: SDG-main/paper/\n",
            "  inflating: SDG-main/paper/SDG_A Simplified and Dynamic Graph Neural Network.pdf  \n",
            "   creating: SDG-main/slides/\n",
            "  inflating: SDG-main/slides/SIGIR'21_SDG_Presentation_Slides.pdf  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Testing Model"
      ],
      "metadata": {
        "id": "-A8jRAIyYy4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the model on Cora_ML Dataset"
      ],
      "metadata": {
        "id": "XGAg097hYPH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!python SDG-main/code/main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiCHAyciRp-I",
        "outputId": "3ab13a26-e9da-478a-9664-230a07f046c4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-27 21:43:01: agnostic_model: {'hiddenunits': [64], 'drop_prob': 0.5, 'propagation': PPRExact()}\n",
            "2024-11-27 21:43:01: PyTorch seed: 1837425462\n",
            "/content/SDG-main/code/sdg/pytorch_code/utils.py:74: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
            "  return torch.sparse.FloatTensor(\n",
            "2024-11-27 21:43:03: Epoch 0: Train loss = 2.00, train acc = 12.9, early stopping loss = 1.96, early stopping acc = 37.8 (0.361 sec)\n",
            "2024-11-27 21:43:03: Epoch 20: Train loss = 1.94, train acc = 71.4, early stopping loss = 1.95, early stopping acc = 55.0 (0.440 sec)\n",
            "2024-11-27 21:43:03: Epoch 40: Train loss = 1.90, train acc = 79.3, early stopping loss = 1.94, early stopping acc = 58.6 (0.265 sec)\n",
            "2024-11-27 21:43:04: Epoch 60: Train loss = 1.84, train acc = 83.6, early stopping loss = 1.92, early stopping acc = 65.2 (0.406 sec)\n",
            "2024-11-27 21:43:04: Epoch 80: Train loss = 1.75, train acc = 89.3, early stopping loss = 1.88, early stopping acc = 71.8 (0.340 sec)\n",
            "2024-11-27 21:43:05: Epoch 100: Train loss = 1.67, train acc = 91.4, early stopping loss = 1.83, early stopping acc = 74.4 (0.406 sec)\n",
            "2024-11-27 21:43:05: Epoch 120: Train loss = 1.58, train acc = 95.7, early stopping loss = 1.77, early stopping acc = 78.2 (0.373 sec)\n",
            "2024-11-27 21:43:05: Epoch 140: Train loss = 1.50, train acc = 94.3, early stopping loss = 1.71, early stopping acc = 77.8 (0.274 sec)\n",
            "2024-11-27 21:43:05: Epoch 160: Train loss = 1.43, train acc = 94.3, early stopping loss = 1.65, early stopping acc = 79.2 (0.207 sec)\n",
            "2024-11-27 21:43:06: Epoch 180: Train loss = 1.35, train acc = 97.9, early stopping loss = 1.60, early stopping acc = 80.2 (0.259 sec)\n",
            "2024-11-27 21:43:06: Epoch 200: Train loss = 1.30, train acc = 96.4, early stopping loss = 1.56, early stopping acc = 79.6 (0.210 sec)\n",
            "2024-11-27 21:43:06: Epoch 220: Train loss = 1.22, train acc = 97.9, early stopping loss = 1.51, early stopping acc = 80.6 (0.239 sec)\n",
            "2024-11-27 21:43:06: Epoch 240: Train loss = 1.20, train acc = 98.6, early stopping loss = 1.48, early stopping acc = 79.6 (0.213 sec)\n",
            "2024-11-27 21:43:07: Epoch 260: Train loss = 1.15, train acc = 97.1, early stopping loss = 1.44, early stopping acc = 80.6 (0.205 sec)\n",
            "2024-11-27 21:43:07: Epoch 280: Train loss = 1.10, train acc = 96.4, early stopping loss = 1.41, early stopping acc = 80.2 (0.225 sec)\n",
            "2024-11-27 21:43:07: Epoch 300: Train loss = 1.06, train acc = 96.4, early stopping loss = 1.38, early stopping acc = 81.4 (0.205 sec)\n",
            "2024-11-27 21:43:07: Epoch 320: Train loss = 1.04, train acc = 98.6, early stopping loss = 1.36, early stopping acc = 80.0 (0.204 sec)\n",
            "2024-11-27 21:43:07: Epoch 340: Train loss = 1.01, train acc = 98.6, early stopping loss = 1.34, early stopping acc = 79.8 (0.227 sec)\n",
            "2024-11-27 21:43:08: Epoch 360: Train loss = 1.01, train acc = 97.9, early stopping loss = 1.31, early stopping acc = 80.8 (0.287 sec)\n",
            "2024-11-27 21:43:08: Epoch 380: Train loss = 0.96, train acc = 98.6, early stopping loss = 1.29, early stopping acc = 81.0 (0.200 sec)\n",
            "2024-11-27 21:43:08: Epoch 400: Train loss = 0.91, train acc = 97.9, early stopping loss = 1.27, early stopping acc = 81.6 (0.209 sec)\n",
            "2024-11-27 21:43:08: Epoch 420: Train loss = 0.91, train acc = 99.3, early stopping loss = 1.25, early stopping acc = 80.6 (0.209 sec)\n",
            "2024-11-27 21:43:09: Epoch 440: Train loss = 0.88, train acc = 99.3, early stopping loss = 1.23, early stopping acc = 80.8 (0.200 sec)\n",
            "2024-11-27 21:43:09: Epoch 460: Train loss = 0.90, train acc = 97.9, early stopping loss = 1.23, early stopping acc = 79.4 (0.211 sec)\n",
            "2024-11-27 21:43:09: Epoch 480: Train loss = 0.83, train acc = 98.6, early stopping loss = 1.20, early stopping acc = 81.2 (0.213 sec)\n",
            "2024-11-27 21:43:09: Epoch 500: Train loss = 0.84, train acc = 98.6, early stopping loss = 1.19, early stopping acc = 81.0 (0.208 sec)\n",
            "2024-11-27 21:43:09: Epoch 520: Train loss = 0.83, train acc = 98.6, early stopping loss = 1.17, early stopping acc = 81.6 (0.199 sec)\n",
            "2024-11-27 21:43:10: Epoch 540: Train loss = 0.80, train acc = 99.3, early stopping loss = 1.16, early stopping acc = 80.0 (0.211 sec)\n",
            "2024-11-27 21:43:10: Epoch 560: Train loss = 0.79, train acc = 99.3, early stopping loss = 1.16, early stopping acc = 80.0 (0.293 sec)\n",
            "2024-11-27 21:43:10: Epoch 580: Train loss = 0.75, train acc = 99.3, early stopping loss = 1.14, early stopping acc = 80.6 (0.314 sec)\n",
            "2024-11-27 21:43:10: Epoch 600: Train loss = 0.75, train acc = 98.6, early stopping loss = 1.14, early stopping acc = 81.4 (0.273 sec)\n",
            "2024-11-27 21:43:11: Epoch 620: Train loss = 0.74, train acc = 98.6, early stopping loss = 1.13, early stopping acc = 80.6 (0.381 sec)\n",
            "2024-11-27 21:43:11: Epoch 640: Train loss = 0.73, train acc = 100.0, early stopping loss = 1.11, early stopping acc = 81.6 (0.299 sec)\n",
            "2024-11-27 21:43:11: Epoch 660: Train loss = 0.72, train acc = 100.0, early stopping loss = 1.09, early stopping acc = 81.2 (0.303 sec)\n",
            "2024-11-27 21:43:12: Epoch 680: Train loss = 0.72, train acc = 97.9, early stopping loss = 1.09, early stopping acc = 81.4 (0.297 sec)\n",
            "2024-11-27 21:43:12: Epoch 700: Train loss = 0.69, train acc = 97.9, early stopping loss = 1.08, early stopping acc = 81.2 (0.300 sec)\n",
            "2024-11-27 21:43:12: Epoch 720: Train loss = 0.70, train acc = 100.0, early stopping loss = 1.08, early stopping acc = 83.0 (0.222 sec)\n",
            "2024-11-27 21:43:12: Epoch 740: Train loss = 0.70, train acc = 99.3, early stopping loss = 1.07, early stopping acc = 80.8 (0.209 sec)\n",
            "2024-11-27 21:43:13: Epoch 760: Train loss = 0.67, train acc = 99.3, early stopping loss = 1.07, early stopping acc = 80.8 (0.203 sec)\n",
            "2024-11-27 21:43:13: Epoch 780: Train loss = 0.65, train acc = 99.3, early stopping loss = 1.05, early stopping acc = 82.0 (0.212 sec)\n",
            "2024-11-27 21:43:13: Epoch 800: Train loss = 0.66, train acc = 100.0, early stopping loss = 1.07, early stopping acc = 82.4 (0.200 sec)\n",
            "2024-11-27 21:43:13: Epoch 820: Train loss = 0.65, train acc = 99.3, early stopping loss = 1.04, early stopping acc = 80.0 (0.203 sec)\n",
            "2024-11-27 21:43:13: Epoch 840: Train loss = 0.63, train acc = 99.3, early stopping loss = 1.04, early stopping acc = 81.8 (0.200 sec)\n",
            "2024-11-27 21:43:14: Epoch 860: Train loss = 0.64, train acc = 98.6, early stopping loss = 1.03, early stopping acc = 80.2 (0.275 sec)\n",
            "2024-11-27 21:43:14: Epoch 880: Train loss = 0.62, train acc = 99.3, early stopping loss = 1.03, early stopping acc = 81.0 (0.211 sec)\n",
            "2024-11-27 21:43:14: Epoch 900: Train loss = 0.61, train acc = 98.6, early stopping loss = 1.01, early stopping acc = 81.6 (0.205 sec)\n",
            "2024-11-27 21:43:14: Epoch 920: Train loss = 0.63, train acc = 99.3, early stopping loss = 1.00, early stopping acc = 82.0 (0.198 sec)\n",
            "2024-11-27 21:43:15: Epoch 940: Train loss = 0.60, train acc = 100.0, early stopping loss = 1.00, early stopping acc = 81.6 (0.204 sec)\n",
            "2024-11-27 21:43:15: Epoch 960: Train loss = 0.59, train acc = 99.3, early stopping loss = 0.99, early stopping acc = 81.2 (0.198 sec)\n",
            "2024-11-27 21:43:15: Epoch 980: Train loss = 0.58, train acc = 100.0, early stopping loss = 0.99, early stopping acc = 82.0 (0.208 sec)\n",
            "2024-11-27 21:43:15: Epoch 1000: Train loss = 0.57, train acc = 99.3, early stopping loss = 0.99, early stopping acc = 80.4 (0.203 sec)\n",
            "2024-11-27 21:43:15: Epoch 1020: Train loss = 0.58, train acc = 97.1, early stopping loss = 1.00, early stopping acc = 80.6 (0.200 sec)\n",
            "2024-11-27 21:43:16: Epoch 1040: Train loss = 0.57, train acc = 99.3, early stopping loss = 0.97, early stopping acc = 83.6 (0.202 sec)\n",
            "2024-11-27 21:43:16: Epoch 1060: Train loss = 0.56, train acc = 100.0, early stopping loss = 0.96, early stopping acc = 82.2 (0.203 sec)\n",
            "2024-11-27 21:43:16: Epoch 1080: Train loss = 0.56, train acc = 99.3, early stopping loss = 0.96, early stopping acc = 80.4 (0.207 sec)\n",
            "2024-11-27 21:43:16: Epoch 1100: Train loss = 0.55, train acc = 100.0, early stopping loss = 0.95, early stopping acc = 81.8 (0.203 sec)\n",
            "2024-11-27 21:43:16: Epoch 1120: Train loss = 0.56, train acc = 98.6, early stopping loss = 0.95, early stopping acc = 82.8 (0.277 sec)\n",
            "2024-11-27 21:43:17: Epoch 1140: Train loss = 0.55, train acc = 100.0, early stopping loss = 0.95, early stopping acc = 82.4 (0.197 sec)\n",
            "2024-11-27 21:43:17: Epoch 1160: Train loss = 0.56, train acc = 100.0, early stopping loss = 0.95, early stopping acc = 81.4 (0.202 sec)\n",
            "2024-11-27 21:43:17: Epoch 1180: Train loss = 0.54, train acc = 99.3, early stopping loss = 0.93, early stopping acc = 82.4 (0.231 sec)\n",
            "2024-11-27 21:43:17: Epoch 1200: Train loss = 0.53, train acc = 99.3, early stopping loss = 0.92, early stopping acc = 81.0 (0.215 sec)\n",
            "2024-11-27 21:43:17: Epoch 1220: Train loss = 0.54, train acc = 99.3, early stopping loss = 0.93, early stopping acc = 83.6 (0.197 sec)\n",
            "2024-11-27 21:43:18: Epoch 1240: Train loss = 0.50, train acc = 100.0, early stopping loss = 0.91, early stopping acc = 83.2 (0.200 sec)\n",
            "2024-11-27 21:43:18: Epoch 1260: Train loss = 0.51, train acc = 100.0, early stopping loss = 0.92, early stopping acc = 82.4 (0.200 sec)\n",
            "2024-11-27 21:43:18: Epoch 1280: Train loss = 0.50, train acc = 100.0, early stopping loss = 0.93, early stopping acc = 81.0 (0.209 sec)\n",
            "2024-11-27 21:43:18: Epoch 1300: Train loss = 0.50, train acc = 99.3, early stopping loss = 0.92, early stopping acc = 81.4 (0.202 sec)\n",
            "2024-11-27 21:43:19: Epoch 1320: Train loss = 0.51, train acc = 98.6, early stopping loss = 0.89, early stopping acc = 82.8 (0.199 sec)\n",
            "2024-11-27 21:43:19: Epoch 1340: Train loss = 0.49, train acc = 100.0, early stopping loss = 0.92, early stopping acc = 80.4 (0.196 sec)\n",
            "2024-11-27 21:43:19: Epoch 1360: Train loss = 0.46, train acc = 100.0, early stopping loss = 0.90, early stopping acc = 82.0 (0.198 sec)\n",
            "2024-11-27 21:43:19: Epoch 1380: Train loss = 0.48, train acc = 99.3, early stopping loss = 0.92, early stopping acc = 81.2 (0.289 sec)\n",
            "2024-11-27 21:43:19: Epoch 1400: Train loss = 0.48, train acc = 100.0, early stopping loss = 0.90, early stopping acc = 81.2 (0.198 sec)\n",
            "2024-11-27 21:43:20: Epoch 1420: Train loss = 0.46, train acc = 100.0, early stopping loss = 0.90, early stopping acc = 81.0 (0.197 sec)\n",
            "2024-11-27 21:43:20: Epoch 1440: Train loss = 0.48, train acc = 100.0, early stopping loss = 0.90, early stopping acc = 82.4 (0.199 sec)\n",
            "2024-11-27 21:43:20: Epoch 1460: Train loss = 0.46, train acc = 100.0, early stopping loss = 0.90, early stopping acc = 80.8 (0.209 sec)\n",
            "2024-11-27 21:43:20: Epoch 1480: Train loss = 0.47, train acc = 100.0, early stopping loss = 0.87, early stopping acc = 83.6 (0.205 sec)\n",
            "2024-11-27 21:43:20: Epoch 1500: Train loss = 0.49, train acc = 100.0, early stopping loss = 0.90, early stopping acc = 80.0 (0.229 sec)\n",
            "2024-11-27 21:43:21: Epoch 1520: Train loss = 0.47, train acc = 99.3, early stopping loss = 0.89, early stopping acc = 81.4 (0.200 sec)\n",
            "2024-11-27 21:43:21: Epoch 1540: Train loss = 0.47, train acc = 98.6, early stopping loss = 0.87, early stopping acc = 82.0 (0.201 sec)\n",
            "2024-11-27 21:43:21: Epoch 1560: Train loss = 0.45, train acc = 100.0, early stopping loss = 0.88, early stopping acc = 81.8 (0.209 sec)\n",
            "2024-11-27 21:43:21: Epoch 1580: Train loss = 0.46, train acc = 100.0, early stopping loss = 0.88, early stopping acc = 81.6 (0.209 sec)\n",
            "2024-11-27 21:43:21: Epoch 1600: Train loss = 0.44, train acc = 100.0, early stopping loss = 0.89, early stopping acc = 81.4 (0.202 sec)\n",
            "2024-11-27 21:43:22: Epoch 1620: Train loss = 0.44, train acc = 100.0, early stopping loss = 0.88, early stopping acc = 81.4 (0.200 sec)\n",
            "2024-11-27 21:43:22: Epoch 1640: Train loss = 0.44, train acc = 100.0, early stopping loss = 0.89, early stopping acc = 80.2 (0.280 sec)\n",
            "2024-11-27 21:43:22: Epoch 1660: Train loss = 0.42, train acc = 100.0, early stopping loss = 0.86, early stopping acc = 81.2 (0.294 sec)\n",
            "2024-11-27 21:43:23: Epoch 1680: Train loss = 0.45, train acc = 100.0, early stopping loss = 0.87, early stopping acc = 81.6 (0.287 sec)\n",
            "2024-11-27 21:43:23: Epoch 1700: Train loss = 0.42, train acc = 100.0, early stopping loss = 0.85, early stopping acc = 82.8 (0.281 sec)\n",
            "2024-11-27 21:43:23: Epoch 1720: Train loss = 0.45, train acc = 100.0, early stopping loss = 0.85, early stopping acc = 83.0 (0.280 sec)\n",
            "2024-11-27 21:43:23: Epoch 1740: Train loss = 0.40, train acc = 100.0, early stopping loss = 0.88, early stopping acc = 80.8 (0.307 sec)\n",
            "2024-11-27 21:43:24: Epoch 1760: Train loss = 0.42, train acc = 100.0, early stopping loss = 0.87, early stopping acc = 80.6 (0.290 sec)\n",
            "2024-11-27 21:43:24: Epoch 1780: Train loss = 0.42, train acc = 99.3, early stopping loss = 0.86, early stopping acc = 80.4 (0.299 sec)\n",
            "2024-11-27 21:43:24: Epoch 1800: Train loss = 0.41, train acc = 100.0, early stopping loss = 0.84, early stopping acc = 80.4 (0.319 sec)\n",
            "2024-11-27 21:43:25: Epoch 1820: Train loss = 0.42, train acc = 100.0, early stopping loss = 0.87, early stopping acc = 80.2 (0.305 sec)\n",
            "2024-11-27 21:43:25: Epoch 1840: Train loss = 0.40, train acc = 100.0, early stopping loss = 0.86, early stopping acc = 81.0 (0.218 sec)\n",
            "2024-11-27 21:43:25: Epoch 1860: Train loss = 0.42, train acc = 100.0, early stopping loss = 0.84, early stopping acc = 81.6 (0.209 sec)\n",
            "2024-11-27 21:43:25: Epoch 1880: Train loss = 0.43, train acc = 100.0, early stopping loss = 0.83, early stopping acc = 81.0 (0.222 sec)\n",
            "2024-11-27 21:43:26: Epoch 1900: Train loss = 0.39, train acc = 100.0, early stopping loss = 0.84, early stopping acc = 80.8 (0.281 sec)\n",
            "2024-11-27 21:43:26: Epoch 1920: Train loss = 0.39, train acc = 100.0, early stopping loss = 0.81, early stopping acc = 82.8 (0.203 sec)\n",
            "2024-11-27 21:43:26: Epoch 1940: Train loss = 0.42, train acc = 100.0, early stopping loss = 0.83, early stopping acc = 82.4 (0.203 sec)\n",
            "2024-11-27 21:43:26: Epoch 1960: Train loss = 0.40, train acc = 100.0, early stopping loss = 0.86, early stopping acc = 81.0 (0.210 sec)\n",
            "2024-11-27 21:43:26: Epoch 1980: Train loss = 0.40, train acc = 100.0, early stopping loss = 0.82, early stopping acc = 82.6 (0.213 sec)\n",
            "2024-11-27 21:43:27: Epoch 2000: Train loss = 0.40, train acc = 100.0, early stopping loss = 0.85, early stopping acc = 82.2 (0.215 sec)\n",
            "2024-11-27 21:43:27: Epoch 2020: Train loss = 0.39, train acc = 100.0, early stopping loss = 0.82, early stopping acc = 83.6 (0.198 sec)\n",
            "2024-11-27 21:43:27: Epoch 2040: Train loss = 0.38, train acc = 100.0, early stopping loss = 0.80, early stopping acc = 84.0 (0.200 sec)\n",
            "2024-11-27 21:43:27: Epoch 2060: Train loss = 0.38, train acc = 100.0, early stopping loss = 0.82, early stopping acc = 83.2 (0.203 sec)\n",
            "2024-11-27 21:43:27: Epoch 2080: Train loss = 0.40, train acc = 100.0, early stopping loss = 0.83, early stopping acc = 83.4 (0.212 sec)\n",
            "2024-11-27 21:43:28: Epoch 2100: Train loss = 0.38, train acc = 99.3, early stopping loss = 0.82, early stopping acc = 82.2 (0.212 sec)\n",
            "2024-11-27 21:43:28: Epoch 2120: Train loss = 0.38, train acc = 100.0, early stopping loss = 0.84, early stopping acc = 82.0 (0.202 sec)\n",
            "2024-11-27 21:43:28: Epoch 2140: Train loss = 0.37, train acc = 100.0, early stopping loss = 0.82, early stopping acc = 82.0 (0.202 sec)\n",
            "2024-11-27 21:43:28: Epoch 2160: Train loss = 0.37, train acc = 100.0, early stopping loss = 0.78, early stopping acc = 83.4 (0.289 sec)\n",
            "2024-11-27 21:43:28: Epoch 2180: Train loss = 0.37, train acc = 100.0, early stopping loss = 0.81, early stopping acc = 81.4 (0.202 sec)\n",
            "2024-11-27 21:43:29: Epoch 2200: Train loss = 0.37, train acc = 100.0, early stopping loss = 0.82, early stopping acc = 81.0 (0.210 sec)\n",
            "2024-11-27 21:43:29: Epoch 2220: Train loss = 0.40, train acc = 100.0, early stopping loss = 0.84, early stopping acc = 82.0 (0.228 sec)\n",
            "2024-11-27 21:43:29: Epoch 2240: Train loss = 0.38, train acc = 100.0, early stopping loss = 0.85, early stopping acc = 80.6 (0.216 sec)\n",
            "2024-11-27 21:43:29: Epoch 2260: Train loss = 0.38, train acc = 100.0, early stopping loss = 0.80, early stopping acc = 83.0 (0.219 sec)\n",
            "2024-11-27 21:43:29: Last epoch: 2261, best epoch: 1955 (27.024 sec)\n",
            "2024-11-27 21:43:29: Early stopping accuracy: 84.4%\n",
            "2024-11-27 21:43:29: Validation accuracy: 84.4%\n",
            "Training PPNP costs: 29.971404552459717 sec.\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/sparse/_index.py:108: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_intXint(row, col, x.flat[0])\n",
            "Generating the new graph costs: 1.8367886543273926 sec.\n",
            "2024-11-27 21:43:32: Fine-tuned model: {'hiddenunits': [64], 'drop_prob': 0.5, 'propagation': SDG()}\n",
            "2024-11-27 21:43:32: PyTorch seed: 2236279788\n",
            "2024-11-27 21:43:32: Epoch 0: Train loss = 0.41, train acc = 99.3, early stopping loss = 1.06, early stopping acc = 74.0 (0.015 sec)\n",
            "2024-11-27 21:43:32: Epoch 20: Train loss = 0.39, train acc = 100.0, early stopping loss = 0.83, early stopping acc = 81.4 (0.346 sec)\n",
            "2024-11-27 21:43:33: Epoch 40: Train loss = 0.37, train acc = 100.0, early stopping loss = 0.82, early stopping acc = 83.6 (0.246 sec)\n",
            "2024-11-27 21:43:33: Epoch 60: Train loss = 0.37, train acc = 100.0, early stopping loss = 0.82, early stopping acc = 81.0 (0.234 sec)\n",
            "2024-11-27 21:43:33: Epoch 80: Train loss = 0.37, train acc = 100.0, early stopping loss = 0.84, early stopping acc = 81.4 (0.212 sec)\n",
            "2024-11-27 21:43:33: Epoch 100: Train loss = 0.37, train acc = 100.0, early stopping loss = 0.80, early stopping acc = 82.8 (0.211 sec)\n",
            "2024-11-27 21:43:34: Epoch 120: Train loss = 0.37, train acc = 100.0, early stopping loss = 0.84, early stopping acc = 80.2 (0.296 sec)\n",
            "2024-11-27 21:43:34: Epoch 140: Train loss = 0.36, train acc = 100.0, early stopping loss = 0.79, early stopping acc = 83.6 (0.201 sec)\n",
            "2024-11-27 21:43:34: Epoch 160: Train loss = 0.36, train acc = 100.0, early stopping loss = 0.79, early stopping acc = 82.4 (0.207 sec)\n",
            "2024-11-27 21:43:34: Epoch 180: Train loss = 0.36, train acc = 99.3, early stopping loss = 0.82, early stopping acc = 81.4 (0.202 sec)\n",
            "2024-11-27 21:43:34: Epoch 200: Train loss = 0.35, train acc = 99.3, early stopping loss = 0.78, early stopping acc = 83.6 (0.211 sec)\n",
            "2024-11-27 21:43:35: Epoch 220: Train loss = 0.36, train acc = 100.0, early stopping loss = 0.80, early stopping acc = 82.2 (0.207 sec)\n",
            "2024-11-27 21:43:35: Epoch 240: Train loss = 0.36, train acc = 98.6, early stopping loss = 0.77, early stopping acc = 82.6 (0.292 sec)\n",
            "2024-11-27 21:43:35: Epoch 260: Train loss = 0.36, train acc = 99.3, early stopping loss = 0.78, early stopping acc = 82.6 (0.303 sec)\n",
            "2024-11-27 21:43:35: Epoch 280: Train loss = 0.33, train acc = 100.0, early stopping loss = 0.76, early stopping acc = 83.4 (0.281 sec)\n",
            "2024-11-27 21:43:36: Epoch 300: Train loss = 0.34, train acc = 100.0, early stopping loss = 0.77, early stopping acc = 81.4 (0.268 sec)\n",
            "2024-11-27 21:43:36: Epoch 320: Train loss = 0.35, train acc = 99.3, early stopping loss = 0.80, early stopping acc = 80.8 (0.276 sec)\n",
            "2024-11-27 21:43:36: Epoch 340: Train loss = 0.33, train acc = 98.6, early stopping loss = 0.80, early stopping acc = 81.0 (0.302 sec)\n",
            "2024-11-27 21:43:37: Epoch 360: Train loss = 0.34, train acc = 100.0, early stopping loss = 0.75, early stopping acc = 83.6 (0.314 sec)\n",
            "2024-11-27 21:43:37: Epoch 380: Train loss = 0.31, train acc = 100.0, early stopping loss = 0.80, early stopping acc = 80.8 (0.407 sec)\n",
            "2024-11-27 21:43:37: Last epoch: 381, best epoch: 3 (5.047 sec)\n",
            "2024-11-27 21:43:37: Early stopping accuracy: 85.4%\n",
            "2024-11-27 21:43:37: Validation accuracy: 84.8%\n",
            "Generating the new graph + Training SDG costs: 7.680777072906494 sec.\n",
            "CPU times: user 342 ms, sys: 44.2 ms, total: 386 ms\n",
            "Wall time: 40.8 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the model on Citeseer Dataset"
      ],
      "metadata": {
        "id": "btTRKuhEYYys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!python SDG-main/code/main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9umqLR2lUK7Y",
        "outputId": "21b98cf4-2e0a-4f61-f511-7b8d2b9a3169"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SDG-main/code/sdg/data/sparsegraph.py:397: UserWarning: 124 self loops removed\n",
            "  warnings.warn(\"{0} self loops removed\".format(num_self_loops))\n",
            "2024-11-27 21:44:54: agnostic_model: {'hiddenunits': [64], 'drop_prob': 0.5, 'propagation': PPRExact()}\n",
            "2024-11-27 21:44:54: PyTorch seed: 1700194395\n",
            "/content/SDG-main/code/sdg/pytorch_code/utils.py:74: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
            "  return torch.sparse.FloatTensor(\n",
            "2024-11-27 21:44:56: Epoch 0: Train loss = 1.85, train acc = 17.5, early stopping loss = 1.81, early stopping acc = 32.6 (0.380 sec)\n",
            "2024-11-27 21:44:56: Epoch 20: Train loss = 1.78, train acc = 53.3, early stopping loss = 1.78, early stopping acc = 55.0 (0.388 sec)\n",
            "2024-11-27 21:44:56: Epoch 40: Train loss = 1.73, train acc = 64.2, early stopping loss = 1.74, early stopping acc = 63.0 (0.396 sec)\n",
            "2024-11-27 21:44:57: Epoch 60: Train loss = 1.64, train acc = 71.7, early stopping loss = 1.68, early stopping acc = 70.8 (0.420 sec)\n",
            "2024-11-27 21:44:57: Epoch 80: Train loss = 1.54, train acc = 77.5, early stopping loss = 1.61, early stopping acc = 72.6 (0.417 sec)\n",
            "2024-11-27 21:44:58: Epoch 100: Train loss = 1.46, train acc = 81.7, early stopping loss = 1.54, early stopping acc = 73.4 (0.384 sec)\n",
            "2024-11-27 21:44:58: Epoch 120: Train loss = 1.35, train acc = 85.8, early stopping loss = 1.50, early stopping acc = 72.2 (0.303 sec)\n",
            "2024-11-27 21:44:58: Epoch 140: Train loss = 1.27, train acc = 87.5, early stopping loss = 1.46, early stopping acc = 72.8 (0.314 sec)\n",
            "2024-11-27 21:44:59: Epoch 160: Train loss = 1.22, train acc = 88.3, early stopping loss = 1.43, early stopping acc = 73.8 (0.305 sec)\n",
            "2024-11-27 21:44:59: Epoch 180: Train loss = 1.18, train acc = 88.3, early stopping loss = 1.41, early stopping acc = 73.0 (0.216 sec)\n",
            "2024-11-27 21:44:59: Epoch 200: Train loss = 1.13, train acc = 91.7, early stopping loss = 1.38, early stopping acc = 72.0 (0.205 sec)\n",
            "2024-11-27 21:44:59: Epoch 220: Train loss = 1.11, train acc = 89.2, early stopping loss = 1.36, early stopping acc = 71.6 (0.242 sec)\n",
            "2024-11-27 21:44:59: Epoch 240: Train loss = 1.05, train acc = 92.5, early stopping loss = 1.35, early stopping acc = 72.4 (0.207 sec)\n",
            "2024-11-27 21:45:00: Epoch 260: Train loss = 1.03, train acc = 91.7, early stopping loss = 1.33, early stopping acc = 73.2 (0.223 sec)\n",
            "2024-11-27 21:45:00: Epoch 280: Train loss = 0.99, train acc = 92.5, early stopping loss = 1.33, early stopping acc = 72.6 (0.205 sec)\n",
            "2024-11-27 21:45:00: Epoch 300: Train loss = 0.93, train acc = 94.2, early stopping loss = 1.31, early stopping acc = 72.8 (0.218 sec)\n",
            "2024-11-27 21:45:00: Epoch 320: Train loss = 0.93, train acc = 95.0, early stopping loss = 1.29, early stopping acc = 71.6 (0.208 sec)\n",
            "2024-11-27 21:45:01: Epoch 340: Train loss = 0.90, train acc = 95.0, early stopping loss = 1.29, early stopping acc = 71.6 (0.201 sec)\n",
            "2024-11-27 21:45:01: Epoch 360: Train loss = 0.88, train acc = 94.2, early stopping loss = 1.29, early stopping acc = 71.8 (0.220 sec)\n",
            "2024-11-27 21:45:01: Epoch 380: Train loss = 0.85, train acc = 98.3, early stopping loss = 1.28, early stopping acc = 71.8 (0.211 sec)\n",
            "2024-11-27 21:45:01: Epoch 400: Train loss = 0.86, train acc = 94.2, early stopping loss = 1.27, early stopping acc = 72.4 (0.229 sec)\n",
            "2024-11-27 21:45:01: Epoch 420: Train loss = 0.84, train acc = 94.2, early stopping loss = 1.25, early stopping acc = 71.8 (0.198 sec)\n",
            "2024-11-27 21:45:02: Epoch 440: Train loss = 0.80, train acc = 95.8, early stopping loss = 1.24, early stopping acc = 72.4 (0.205 sec)\n",
            "2024-11-27 21:45:02: Epoch 460: Train loss = 0.75, train acc = 97.5, early stopping loss = 1.25, early stopping acc = 71.6 (0.199 sec)\n",
            "2024-11-27 21:45:02: Epoch 480: Train loss = 0.76, train acc = 96.7, early stopping loss = 1.23, early stopping acc = 71.2 (0.219 sec)\n",
            "2024-11-27 21:45:02: Epoch 500: Train loss = 0.75, train acc = 98.3, early stopping loss = 1.23, early stopping acc = 71.2 (0.209 sec)\n",
            "2024-11-27 21:45:02: Epoch 520: Train loss = 0.72, train acc = 96.7, early stopping loss = 1.24, early stopping acc = 71.4 (0.197 sec)\n",
            "2024-11-27 21:45:03: Epoch 540: Train loss = 0.73, train acc = 98.3, early stopping loss = 1.24, early stopping acc = 70.8 (0.199 sec)\n",
            "2024-11-27 21:45:03: Epoch 560: Train loss = 0.70, train acc = 98.3, early stopping loss = 1.23, early stopping acc = 71.4 (0.195 sec)\n",
            "2024-11-27 21:45:03: Epoch 580: Train loss = 0.71, train acc = 95.0, early stopping loss = 1.21, early stopping acc = 71.4 (0.200 sec)\n",
            "2024-11-27 21:45:03: Epoch 600: Train loss = 0.67, train acc = 98.3, early stopping loss = 1.21, early stopping acc = 71.2 (0.208 sec)\n",
            "2024-11-27 21:45:03: Epoch 620: Train loss = 0.70, train acc = 95.8, early stopping loss = 1.21, early stopping acc = 70.8 (0.243 sec)\n",
            "2024-11-27 21:45:04: Epoch 640: Train loss = 0.67, train acc = 98.3, early stopping loss = 1.20, early stopping acc = 72.0 (0.195 sec)\n",
            "2024-11-27 21:45:04: Epoch 660: Train loss = 0.65, train acc = 96.7, early stopping loss = 1.20, early stopping acc = 71.2 (0.285 sec)\n",
            "2024-11-27 21:45:04: Epoch 680: Train loss = 0.62, train acc = 99.2, early stopping loss = 1.19, early stopping acc = 70.0 (0.265 sec)\n",
            "2024-11-27 21:45:04: Epoch 700: Train loss = 0.63, train acc = 99.2, early stopping loss = 1.20, early stopping acc = 71.8 (0.278 sec)\n",
            "2024-11-27 21:45:05: Epoch 720: Train loss = 0.60, train acc = 97.5, early stopping loss = 1.19, early stopping acc = 71.2 (0.267 sec)\n",
            "2024-11-27 21:45:05: Epoch 740: Train loss = 0.61, train acc = 98.3, early stopping loss = 1.18, early stopping acc = 70.8 (0.274 sec)\n",
            "2024-11-27 21:45:05: Epoch 760: Train loss = 0.61, train acc = 100.0, early stopping loss = 1.22, early stopping acc = 69.8 (0.271 sec)\n",
            "2024-11-27 21:45:06: Epoch 780: Train loss = 0.62, train acc = 97.5, early stopping loss = 1.20, early stopping acc = 69.6 (0.288 sec)\n",
            "2024-11-27 21:45:06: Epoch 800: Train loss = 0.59, train acc = 100.0, early stopping loss = 1.19, early stopping acc = 70.0 (0.291 sec)\n",
            "2024-11-27 21:45:06: Epoch 820: Train loss = 0.60, train acc = 97.5, early stopping loss = 1.18, early stopping acc = 71.4 (0.301 sec)\n",
            "2024-11-27 21:45:07: Epoch 840: Train loss = 0.58, train acc = 96.7, early stopping loss = 1.18, early stopping acc = 70.0 (0.311 sec)\n",
            "2024-11-27 21:45:07: Epoch 860: Train loss = 0.57, train acc = 98.3, early stopping loss = 1.19, early stopping acc = 69.8 (0.239 sec)\n",
            "2024-11-27 21:45:07: Epoch 880: Train loss = 0.53, train acc = 98.3, early stopping loss = 1.18, early stopping acc = 70.6 (0.202 sec)\n",
            "2024-11-27 21:45:07: Epoch 900: Train loss = 0.57, train acc = 98.3, early stopping loss = 1.19, early stopping acc = 70.0 (0.205 sec)\n",
            "2024-11-27 21:45:07: Epoch 920: Train loss = 0.57, train acc = 98.3, early stopping loss = 1.18, early stopping acc = 70.0 (0.196 sec)\n",
            "2024-11-27 21:45:08: Epoch 940: Train loss = 0.56, train acc = 100.0, early stopping loss = 1.18, early stopping acc = 69.8 (0.210 sec)\n",
            "2024-11-27 21:45:08: Epoch 960: Train loss = 0.54, train acc = 100.0, early stopping loss = 1.20, early stopping acc = 69.2 (0.193 sec)\n",
            "2024-11-27 21:45:08: Epoch 980: Train loss = 0.56, train acc = 98.3, early stopping loss = 1.23, early stopping acc = 67.8 (0.195 sec)\n",
            "2024-11-27 21:45:08: Epoch 1000: Train loss = 0.53, train acc = 100.0, early stopping loss = 1.16, early stopping acc = 71.2 (0.197 sec)\n",
            "2024-11-27 21:45:08: Last epoch: 1003, best epoch: 369 (12.856 sec)\n",
            "2024-11-27 21:45:08: Early stopping accuracy: 73.8%\n",
            "2024-11-27 21:45:08: Validation accuracy: 71.4%\n",
            "Training PPNP costs: 14.84458327293396 sec.\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/sparse/_index.py:108: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_intXint(row, col, x.flat[0])\n",
            "Generating the new graph costs: 1.2812747955322266 sec.\n",
            "2024-11-27 21:45:10: Fine-tuned model: {'hiddenunits': [64], 'drop_prob': 0.5, 'propagation': SDG()}\n",
            "2024-11-27 21:45:10: PyTorch seed: 1089905768\n",
            "2024-11-27 21:45:10: Epoch 0: Train loss = 0.87, train acc = 94.2, early stopping loss = 1.27, early stopping acc = 72.6 (0.015 sec)\n",
            "2024-11-27 21:45:10: Epoch 20: Train loss = 0.85, train acc = 96.7, early stopping loss = 1.27, early stopping acc = 72.8 (0.301 sec)\n",
            "2024-11-27 21:45:11: Epoch 40: Train loss = 0.83, train acc = 94.2, early stopping loss = 1.24, early stopping acc = 72.2 (0.344 sec)\n",
            "2024-11-27 21:45:11: Epoch 60: Train loss = 0.75, train acc = 96.7, early stopping loss = 1.24, early stopping acc = 71.2 (0.429 sec)\n",
            "2024-11-27 21:45:11: Epoch 80: Train loss = 0.74, train acc = 95.8, early stopping loss = 1.23, early stopping acc = 72.0 (0.298 sec)\n",
            "2024-11-27 21:45:11: Epoch 100: Train loss = 0.71, train acc = 95.8, early stopping loss = 1.21, early stopping acc = 71.8 (0.215 sec)\n",
            "2024-11-27 21:45:12: Epoch 120: Train loss = 0.66, train acc = 97.5, early stopping loss = 1.20, early stopping acc = 70.8 (0.215 sec)\n",
            "2024-11-27 21:45:12: Epoch 140: Train loss = 0.68, train acc = 97.5, early stopping loss = 1.22, early stopping acc = 70.8 (0.198 sec)\n",
            "2024-11-27 21:45:12: Epoch 160: Train loss = 0.67, train acc = 94.2, early stopping loss = 1.20, early stopping acc = 71.0 (0.202 sec)\n",
            "2024-11-27 21:45:12: Epoch 180: Train loss = 0.65, train acc = 99.2, early stopping loss = 1.21, early stopping acc = 70.6 (0.196 sec)\n",
            "2024-11-27 21:45:12: Epoch 200: Train loss = 0.64, train acc = 99.2, early stopping loss = 1.19, early stopping acc = 71.2 (0.200 sec)\n",
            "2024-11-27 21:45:13: Epoch 220: Train loss = 0.64, train acc = 97.5, early stopping loss = 1.17, early stopping acc = 71.8 (0.218 sec)\n",
            "2024-11-27 21:45:13: Epoch 240: Train loss = 0.64, train acc = 98.3, early stopping loss = 1.20, early stopping acc = 71.8 (0.198 sec)\n",
            "2024-11-27 21:45:13: Epoch 260: Train loss = 0.62, train acc = 100.0, early stopping loss = 1.18, early stopping acc = 71.0 (0.201 sec)\n",
            "2024-11-27 21:45:13: Epoch 280: Train loss = 0.61, train acc = 98.3, early stopping loss = 1.19, early stopping acc = 69.6 (0.196 sec)\n",
            "2024-11-27 21:45:14: Epoch 300: Train loss = 0.58, train acc = 96.7, early stopping loss = 1.17, early stopping acc = 70.6 (0.207 sec)\n",
            "2024-11-27 21:45:14: Epoch 320: Train loss = 0.59, train acc = 94.2, early stopping loss = 1.18, early stopping acc = 71.0 (0.284 sec)\n",
            "2024-11-27 21:45:14: Epoch 340: Train loss = 0.56, train acc = 95.8, early stopping loss = 1.16, early stopping acc = 70.0 (0.194 sec)\n",
            "2024-11-27 21:45:14: Epoch 360: Train loss = 0.56, train acc = 100.0, early stopping loss = 1.19, early stopping acc = 69.6 (0.203 sec)\n",
            "2024-11-27 21:45:14: Epoch 380: Train loss = 0.56, train acc = 97.5, early stopping loss = 1.16, early stopping acc = 70.0 (0.196 sec)\n",
            "2024-11-27 21:45:15: Epoch 400: Train loss = 0.55, train acc = 96.7, early stopping loss = 1.18, early stopping acc = 69.4 (0.198 sec)\n",
            "2024-11-27 21:45:15: Epoch 420: Train loss = 0.51, train acc = 100.0, early stopping loss = 1.18, early stopping acc = 70.4 (0.203 sec)\n",
            "2024-11-27 21:45:15: Epoch 440: Train loss = 0.53, train acc = 98.3, early stopping loss = 1.17, early stopping acc = 69.4 (0.196 sec)\n",
            "2024-11-27 21:45:15: Epoch 460: Train loss = 0.50, train acc = 99.2, early stopping loss = 1.15, early stopping acc = 69.0 (0.198 sec)\n",
            "2024-11-27 21:45:15: Epoch 480: Train loss = 0.52, train acc = 98.3, early stopping loss = 1.17, early stopping acc = 69.6 (0.198 sec)\n",
            "2024-11-27 21:45:16: Epoch 500: Train loss = 0.51, train acc = 98.3, early stopping loss = 1.17, early stopping acc = 69.6 (0.194 sec)\n",
            "2024-11-27 21:45:16: Epoch 520: Train loss = 0.51, train acc = 98.3, early stopping loss = 1.14, early stopping acc = 70.2 (0.204 sec)\n",
            "2024-11-27 21:45:16: Epoch 540: Train loss = 0.49, train acc = 98.3, early stopping loss = 1.18, early stopping acc = 69.6 (0.204 sec)\n",
            "2024-11-27 21:45:16: Epoch 560: Train loss = 0.49, train acc = 99.2, early stopping loss = 1.16, early stopping acc = 69.6 (0.200 sec)\n",
            "2024-11-27 21:45:16: Epoch 580: Train loss = 0.49, train acc = 100.0, early stopping loss = 1.18, early stopping acc = 69.4 (0.276 sec)\n",
            "2024-11-27 21:45:17: Last epoch: 586, best epoch: 66 (6.642 sec)\n",
            "2024-11-27 21:45:17: Early stopping accuracy: 73.2%\n",
            "2024-11-27 21:45:17: Validation accuracy: 70.0%\n",
            "Generating the new graph + Training SDG costs: 8.36123275756836 sec.\n",
            "CPU times: user 222 ms, sys: 34.8 ms, total: 257 ms\n",
            "Wall time: 25.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the model on Pubmed Dataset"
      ],
      "metadata": {
        "id": "27Kn2RjgYjjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!python SDG-main/code/main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ0NE-hfVPT3",
        "outputId": "67b7c135-dd3a-42e8-ab33-2d7f080a397a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-27 21:55:07: agnostic_model: {'hiddenunits': [64], 'drop_prob': 0.5, 'propagation': PPRExact()}\n",
            "2024-11-27 21:55:07: PyTorch seed: 554409738\n",
            "/content/SDG-main/code/sdg/pytorch_code/utils.py:74: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
            "  return torch.sparse.FloatTensor(\n",
            "2024-11-27 21:55:09: Epoch 0: Train loss = 1.15, train acc = 33.3, early stopping loss = 1.12, early stopping acc = 65.6 (0.421 sec)\n",
            "2024-11-27 21:55:25: Epoch 20: Train loss = 1.07, train acc = 83.3, early stopping loss = 1.08, early stopping acc = 79.4 (15.290 sec)\n",
            "2024-11-27 21:55:34: Epoch 40: Train loss = 0.98, train acc = 81.7, early stopping loss = 1.02, early stopping acc = 80.4 (9.456 sec)\n",
            "2024-11-27 21:55:36: Epoch 60: Train loss = 0.88, train acc = 90.0, early stopping loss = 0.96, early stopping acc = 80.6 (1.879 sec)\n",
            "2024-11-27 21:55:43: Epoch 80: Train loss = 0.80, train acc = 91.7, early stopping loss = 0.91, early stopping acc = 81.4 (6.666 sec)\n",
            "2024-11-27 21:55:51: Epoch 100: Train loss = 0.73, train acc = 96.7, early stopping loss = 0.87, early stopping acc = 80.6 (8.806 sec)\n",
            "2024-11-27 21:55:52: Epoch 120: Train loss = 0.68, train acc = 95.0, early stopping loss = 0.84, early stopping acc = 79.0 (0.730 sec)\n",
            "2024-11-27 21:55:53: Epoch 140: Train loss = 0.62, train acc = 96.7, early stopping loss = 0.81, early stopping acc = 81.2 (0.716 sec)\n",
            "2024-11-27 21:55:54: Epoch 160: Train loss = 0.61, train acc = 95.0, early stopping loss = 0.79, early stopping acc = 80.6 (0.716 sec)\n",
            "2024-11-27 21:55:54: Epoch 180: Train loss = 0.58, train acc = 98.3, early stopping loss = 0.77, early stopping acc = 81.0 (0.714 sec)\n",
            "2024-11-27 21:55:57: Epoch 200: Train loss = 0.53, train acc = 98.3, early stopping loss = 0.75, early stopping acc = 81.0 (2.254 sec)\n",
            "2024-11-27 21:55:57: Epoch 220: Train loss = 0.53, train acc = 98.3, early stopping loss = 0.74, early stopping acc = 81.2 (0.740 sec)\n",
            "2024-11-27 21:55:59: Epoch 240: Train loss = 0.49, train acc = 98.3, early stopping loss = 0.73, early stopping acc = 80.8 (1.857 sec)\n",
            "2024-11-27 21:56:00: Epoch 260: Train loss = 0.46, train acc = 98.3, early stopping loss = 0.74, early stopping acc = 78.6 (0.715 sec)\n",
            "2024-11-27 21:56:02: Epoch 280: Train loss = 0.44, train acc = 98.3, early stopping loss = 0.72, early stopping acc = 80.0 (1.851 sec)\n",
            "2024-11-27 21:56:02: Epoch 300: Train loss = 0.45, train acc = 100.0, early stopping loss = 0.69, early stopping acc = 81.0 (0.719 sec)\n",
            "2024-11-27 21:56:03: Epoch 320: Train loss = 0.42, train acc = 98.3, early stopping loss = 0.69, early stopping acc = 81.4 (0.713 sec)\n",
            "2024-11-27 21:56:04: Epoch 340: Train loss = 0.42, train acc = 100.0, early stopping loss = 0.70, early stopping acc = 78.8 (0.715 sec)\n",
            "2024-11-27 21:56:05: Epoch 360: Train loss = 0.43, train acc = 98.3, early stopping loss = 0.68, early stopping acc = 81.2 (0.713 sec)\n",
            "2024-11-27 21:56:05: Epoch 380: Train loss = 0.40, train acc = 100.0, early stopping loss = 0.67, early stopping acc = 82.0 (0.718 sec)\n",
            "2024-11-27 21:56:06: Epoch 400: Train loss = 0.39, train acc = 100.0, early stopping loss = 0.69, early stopping acc = 82.0 (0.715 sec)\n",
            "2024-11-27 21:56:07: Epoch 420: Train loss = 0.36, train acc = 98.3, early stopping loss = 0.64, early stopping acc = 83.0 (0.715 sec)\n",
            "2024-11-27 21:56:07: Epoch 440: Train loss = 0.37, train acc = 98.3, early stopping loss = 0.65, early stopping acc = 82.2 (0.773 sec)\n",
            "2024-11-27 21:56:10: Epoch 460: Train loss = 0.36, train acc = 100.0, early stopping loss = 0.63, early stopping acc = 82.2 (2.235 sec)\n",
            "2024-11-27 21:56:10: Epoch 480: Train loss = 0.34, train acc = 98.3, early stopping loss = 0.67, early stopping acc = 80.4 (0.717 sec)\n",
            "2024-11-27 21:56:11: Epoch 500: Train loss = 0.36, train acc = 98.3, early stopping loss = 0.62, early stopping acc = 82.0 (0.716 sec)\n",
            "2024-11-27 21:56:12: Epoch 520: Train loss = 0.32, train acc = 100.0, early stopping loss = 0.67, early stopping acc = 79.4 (0.719 sec)\n",
            "2024-11-27 21:56:13: Epoch 540: Train loss = 0.35, train acc = 96.7, early stopping loss = 0.63, early stopping acc = 82.2 (0.731 sec)\n",
            "2024-11-27 21:56:13: Epoch 560: Train loss = 0.34, train acc = 100.0, early stopping loss = 0.63, early stopping acc = 81.0 (0.717 sec)\n",
            "2024-11-27 21:56:14: Epoch 580: Train loss = 0.32, train acc = 100.0, early stopping loss = 0.61, early stopping acc = 82.4 (0.715 sec)\n",
            "2024-11-27 21:56:15: Epoch 600: Train loss = 0.30, train acc = 100.0, early stopping loss = 0.63, early stopping acc = 81.6 (0.794 sec)\n",
            "2024-11-27 21:56:16: Epoch 620: Train loss = 0.31, train acc = 100.0, early stopping loss = 0.60, early stopping acc = 82.6 (0.718 sec)\n",
            "2024-11-27 21:56:16: Epoch 640: Train loss = 0.31, train acc = 100.0, early stopping loss = 0.67, early stopping acc = 77.4 (0.717 sec)\n",
            "2024-11-27 21:56:17: Epoch 660: Train loss = 0.31, train acc = 100.0, early stopping loss = 0.64, early stopping acc = 79.0 (0.716 sec)\n",
            "2024-11-27 21:56:19: Epoch 680: Train loss = 0.31, train acc = 100.0, early stopping loss = 0.60, early stopping acc = 83.0 (1.862 sec)\n",
            "2024-11-27 21:56:20: Epoch 700: Train loss = 0.28, train acc = 98.3, early stopping loss = 0.62, early stopping acc = 81.8 (0.745 sec)\n",
            "2024-11-27 21:56:20: Epoch 720: Train loss = 0.30, train acc = 100.0, early stopping loss = 0.59, early stopping acc = 82.0 (0.778 sec)\n",
            "2024-11-27 21:56:21: Epoch 740: Train loss = 0.27, train acc = 100.0, early stopping loss = 0.62, early stopping acc = 80.8 (0.784 sec)\n",
            "2024-11-27 21:56:24: Epoch 760: Train loss = 0.29, train acc = 100.0, early stopping loss = 0.59, early stopping acc = 82.4 (3.146 sec)\n",
            "2024-11-27 21:56:25: Epoch 780: Train loss = 0.27, train acc = 100.0, early stopping loss = 0.61, early stopping acc = 82.2 (0.721 sec)\n",
            "2024-11-27 21:56:26: Epoch 800: Train loss = 0.28, train acc = 100.0, early stopping loss = 0.58, early stopping acc = 83.2 (0.723 sec)\n",
            "2024-11-27 21:56:26: Epoch 820: Train loss = 0.25, train acc = 100.0, early stopping loss = 0.59, early stopping acc = 82.8 (0.720 sec)\n",
            "2024-11-27 21:56:27: Epoch 840: Train loss = 0.26, train acc = 98.3, early stopping loss = 0.60, early stopping acc = 81.2 (0.719 sec)\n",
            "2024-11-27 21:56:28: Epoch 860: Train loss = 0.25, train acc = 100.0, early stopping loss = 0.62, early stopping acc = 79.0 (0.797 sec)\n",
            "2024-11-27 21:56:29: Epoch 880: Train loss = 0.25, train acc = 100.0, early stopping loss = 0.61, early stopping acc = 79.2 (0.729 sec)\n",
            "2024-11-27 21:56:29: Epoch 900: Train loss = 0.24, train acc = 100.0, early stopping loss = 0.58, early stopping acc = 83.2 (0.718 sec)\n",
            "2024-11-27 21:56:30: Epoch 920: Train loss = 0.26, train acc = 100.0, early stopping loss = 0.62, early stopping acc = 79.4 (0.725 sec)\n",
            "2024-11-27 21:56:31: Epoch 940: Train loss = 0.27, train acc = 100.0, early stopping loss = 0.61, early stopping acc = 81.4 (0.727 sec)\n",
            "2024-11-27 21:56:32: Epoch 960: Train loss = 0.24, train acc = 100.0, early stopping loss = 0.60, early stopping acc = 79.8 (0.720 sec)\n",
            "2024-11-27 21:56:32: Epoch 980: Train loss = 0.27, train acc = 98.3, early stopping loss = 0.58, early stopping acc = 82.2 (0.775 sec)\n",
            "2024-11-27 21:56:33: Epoch 1000: Train loss = 0.25, train acc = 100.0, early stopping loss = 0.58, early stopping acc = 82.4 (0.779 sec)\n",
            "2024-11-27 21:56:34: Epoch 1020: Train loss = 0.24, train acc = 100.0, early stopping loss = 0.57, early stopping acc = 81.6 (0.797 sec)\n",
            "2024-11-27 21:56:35: Epoch 1040: Train loss = 0.25, train acc = 100.0, early stopping loss = 0.57, early stopping acc = 81.0 (0.746 sec)\n",
            "2024-11-27 21:56:35: Epoch 1060: Train loss = 0.23, train acc = 100.0, early stopping loss = 0.58, early stopping acc = 81.4 (0.727 sec)\n",
            "2024-11-27 21:56:36: Epoch 1080: Train loss = 0.25, train acc = 100.0, early stopping loss = 0.60, early stopping acc = 80.4 (0.725 sec)\n",
            "2024-11-27 21:56:37: Epoch 1100: Train loss = 0.22, train acc = 100.0, early stopping loss = 0.61, early stopping acc = 78.2 (0.725 sec)\n",
            "2024-11-27 21:56:38: Epoch 1120: Train loss = 0.24, train acc = 100.0, early stopping loss = 0.62, early stopping acc = 78.6 (0.722 sec)\n",
            "2024-11-27 21:56:38: Epoch 1140: Train loss = 0.22, train acc = 100.0, early stopping loss = 0.56, early stopping acc = 81.8 (0.726 sec)\n",
            "2024-11-27 21:56:39: Epoch 1160: Train loss = 0.25, train acc = 98.3, early stopping loss = 0.56, early stopping acc = 83.4 (0.723 sec)\n",
            "2024-11-27 21:56:40: Epoch 1180: Train loss = 0.23, train acc = 100.0, early stopping loss = 0.66, early stopping acc = 76.0 (0.722 sec)\n",
            "2024-11-27 21:56:40: Epoch 1200: Train loss = 0.24, train acc = 98.3, early stopping loss = 0.60, early stopping acc = 79.4 (0.725 sec)\n",
            "2024-11-27 21:56:41: Epoch 1220: Train loss = 0.23, train acc = 100.0, early stopping loss = 0.59, early stopping acc = 80.4 (0.727 sec)\n",
            "2024-11-27 21:56:42: Epoch 1240: Train loss = 0.21, train acc = 100.0, early stopping loss = 0.56, early stopping acc = 81.8 (0.728 sec)\n",
            "2024-11-27 21:56:43: Epoch 1260: Train loss = 0.22, train acc = 100.0, early stopping loss = 0.61, early stopping acc = 79.8 (0.723 sec)\n",
            "2024-11-27 21:56:43: Epoch 1280: Train loss = 0.25, train acc = 98.3, early stopping loss = 0.60, early stopping acc = 78.4 (0.734 sec)\n",
            "2024-11-27 21:56:43: Last epoch: 1282, best epoch: 744 (94.578 sec)\n",
            "2024-11-27 21:56:44: Early stopping accuracy: 84.2%\n",
            "2024-11-27 21:56:44: Validation accuracy: 81.0%\n",
            "Training PPNP costs: 656.9569411277771 sec.\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/sparse/_index.py:108: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_intXint(row, col, x.flat[0])\n",
            "Generating the new graph costs: 555.0970721244812 sec.\n",
            "2024-11-27 22:06:45: Fine-tuned model: {'hiddenunits': [64], 'drop_prob': 0.5, 'propagation': SDG()}\n",
            "2024-11-27 22:06:45: PyTorch seed: 4232149626\n",
            "2024-11-27 22:06:45: Epoch 0: Train loss = 0.28, train acc = 100.0, early stopping loss = 0.74, early stopping acc = 74.4 (0.061 sec)\n",
            "2024-11-27 22:06:49: Epoch 20: Train loss = 0.28, train acc = 98.3, early stopping loss = 0.63, early stopping acc = 80.0 (3.606 sec)\n",
            "2024-11-27 22:06:50: Epoch 40: Train loss = 0.25, train acc = 100.0, early stopping loss = 0.63, early stopping acc = 79.0 (0.724 sec)\n",
            "2024-11-27 22:06:50: Epoch 60: Train loss = 0.28, train acc = 98.3, early stopping loss = 0.59, early stopping acc = 82.0 (0.720 sec)\n",
            "2024-11-27 22:06:51: Epoch 80: Train loss = 0.29, train acc = 100.0, early stopping loss = 0.58, early stopping acc = 82.4 (0.724 sec)\n",
            "2024-11-27 22:06:52: Epoch 100: Train loss = 0.25, train acc = 100.0, early stopping loss = 0.60, early stopping acc = 80.0 (0.806 sec)\n",
            "2024-11-27 22:06:53: Epoch 120: Train loss = 0.23, train acc = 100.0, early stopping loss = 0.61, early stopping acc = 79.6 (0.726 sec)\n",
            "2024-11-27 22:06:53: Epoch 140: Train loss = 0.22, train acc = 100.0, early stopping loss = 0.55, early stopping acc = 82.8 (0.725 sec)\n",
            "2024-11-27 22:06:55: Epoch 160: Train loss = 0.23, train acc = 100.0, early stopping loss = 0.63, early stopping acc = 79.8 (1.856 sec)\n",
            "2024-11-27 22:06:56: Epoch 180: Train loss = 0.21, train acc = 100.0, early stopping loss = 0.55, early stopping acc = 82.8 (0.726 sec)\n",
            "2024-11-27 22:06:57: Epoch 200: Train loss = 0.23, train acc = 100.0, early stopping loss = 0.59, early stopping acc = 78.6 (0.727 sec)\n",
            "2024-11-27 22:06:57: Epoch 220: Train loss = 0.21, train acc = 100.0, early stopping loss = 0.55, early stopping acc = 83.6 (0.730 sec)\n",
            "2024-11-27 22:07:01: Epoch 240: Train loss = 0.22, train acc = 100.0, early stopping loss = 0.55, early stopping acc = 82.8 (3.598 sec)\n",
            "2024-11-27 22:07:02: Epoch 260: Train loss = 0.20, train acc = 100.0, early stopping loss = 0.63, early stopping acc = 75.8 (0.727 sec)\n",
            "2024-11-27 22:07:03: Epoch 280: Train loss = 0.22, train acc = 98.3, early stopping loss = 0.56, early stopping acc = 83.0 (0.724 sec)\n",
            "2024-11-27 22:07:03: Epoch 300: Train loss = 0.19, train acc = 100.0, early stopping loss = 0.58, early stopping acc = 80.0 (0.727 sec)\n",
            "2024-11-27 22:07:04: Epoch 320: Train loss = 0.20, train acc = 100.0, early stopping loss = 0.56, early stopping acc = 82.0 (0.731 sec)\n",
            "2024-11-27 22:07:04: Last epoch: 330, best epoch: 230 (19.000 sec)\n",
            "2024-11-27 22:07:05: Early stopping accuracy: 84.0%\n",
            "2024-11-27 22:07:05: Validation accuracy: 81.6%\n",
            "Generating the new graph + Training SDG costs: 620.9016971588135 sec.\n",
            "CPU times: user 7.67 s, sys: 1.11 s, total: 8.78 s\n",
            "Wall time: 21min 20s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Conclusion"
      ],
      "metadata": {
        "id": "LBpoSM8lYrFt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "model was tested on various datasets, yielding the following results:\n",
        "\n",
        "##Cora_ML Dataset:\n",
        "For the agnostic model, training commenced at 21:43:01, with the initial training accuracy at 12.9% and a loss of 2.00. Over the course of 2261 epochs, the model exhibited a remarkable improvement, achieving a training accuracy of 100% by epoch 640 and maintaining high performance with an early stopping accuracy of 84.4% and a validation accuracy of 84.4%. The total training time for the PPNP was approximately 29.97 seconds, with the entire training process taking around 40.8 seconds.\n",
        "\n",
        "Subsequently, the fine-tuned model was initialized at 21:43:32, where it quickly reached a training accuracy of 100% by epoch 20. The model ultimately achieved an early stopping accuracy of 85.4% and a validation accuracy of 84.8%, with the training process for the SDG model costing approximately 7.68 seconds\n",
        "\n",
        "##Citeseer Dataset:\n",
        "The training for the agnostic model began at 21:44:54, with an initial training accuracy of 17.5% and a loss of 1.85. Over the course of 1003 epochs, the model demonstrated substantial improvement, achieving a peak training accuracy of 100% by epoch 940. The early stopping accuracy settled at 73.8%, while the validation accuracy reached 71.4%. The total training time for the PPNP was approximately 14.84 seconds, with the entire process taking around 25.6 seconds.\n",
        "\n",
        "Following this, the fine-tuned model was initialized at 21:45:10, where it quickly reached a training accuracy of 94.2% in the first epoch. Throughout its training, it maintained competitive performance, achieving an early stopping accuracy of 73.2% and a validation accuracy of 70.0%. The combined time for generating the new graph and training the SDG model was approximately 8.36 seconds.\n",
        "\n",
        "##Pubmed Dataset:\n",
        "The training for the agnostic model commenced at 21:55:07, starting with a training accuracy of 33.3% and a loss of 1.15. Over the course of 1282 epochs, the model showed impressive improvement, achieving a peak training accuracy of 100% by epoch 700. The early stopping accuracy reached 84.2%, with a validation accuracy of 81.0%. The total time taken for training the PPNP was approximately 656.96 seconds, while the entire process, including graph generation, took about 21 minutes and 20 seconds.\n",
        "\n",
        "Following this, the fine-tuned model was initialized at 22:06:45, where it began with a training accuracy of 100% in the first epoch. Throughout its training, it maintained high performance, achieving an early stopping accuracy of 84.0% and a validation accuracy of 81.6%. The total time for generating the new graph and training the SDG model was approximately 620.90 seconds.\n",
        "\n",
        "#Insights:\n",
        "\n",
        "The model demonstrated high training accuracy across epochs, achieving stable validation accuracy for Cora_ML, Citerseer and pubmed Datasets.\n",
        "The only problem is that the time the model took to process the pubmed showed significant difference from what the model reported(the difference in others was milliseconds)."
      ],
      "metadata": {
        "id": "WVDIhH3fd7px"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gxGrzT4fidDA"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}